{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning CartPole Exercise",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mgyong/examples/blob/master/Q_Learning_CartPole_Exercise.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "iTueGwFWcDK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q-Learning CartPole Exercise\n",
        "\n",
        "go/rl-exercise-qlearning\n",
        "(solution code at go/rl-exercise-qlearning-solution)\n",
        "\n",
        "Last updated 18 Aug 2017\n",
        "\n",
        "Author: Ethan Holly (eholly@)\n",
        "\n",
        "Thanks Charles Weill (weill@) for implementing environment visualization in colab!\n",
        "\n",
        "In this exercise, you will implement and experiment with a Q-Learning RL algorithm and environment.\n",
        "\n",
        "The environment is CartPole: from OpenAI Gym https://gym.openai.com/envs/CartPole-v0\n",
        "\n",
        "## Setup\n",
        "\n",
        "On your corp machine, run this colab kernel. If you want to render full episodes, you will need to run this directly from your physical machine. If you only want to train and plot performance, you can run this through an ssh connection:\n",
        "  - /google/data/ro/users/eh/eholly/www/gym_notebook.par\n",
        "\n",
        "If on laptop, create ssh tunnel to your_corp_machine:8888 :\n",
        "  - https://g3doc.corp.google.com/company/teams/colab/gettingstarted/index.md?cl=head#remote-access-to-local-runtimes-using-ssh-port-forwarding\n",
        "  - $ ssh <your_corp_machine> -L 8888:localhost:8888\n",
        "\n",
        "Click CONNECT on this Colab page and connect to local runtime at port 8888.\n",
        "\n",
        "Skim through the following code blocks to gain an idea of how they work, run the training block to observe the experiment, and consider the questions below.\n",
        "\n",
        "## Exercise\n",
        "\n",
        "The following code blocks contain the parts of the DQN algorithm. Fill in the parts marked TODO, and then run the training block below. Training should take up to 2 min, and your performance curve should approach 200 reward per episode. Then investigate the questions next to the training block.\n",
        "\n",
        "DQN stands for Deep Q-Networks, which is an algorithm that comes from DeepMind's first Nature paper: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "\n",
        "## Extra Challenges:\n",
        "\n",
        "* Try to implement some more complex DQN features:\n",
        "  * **Double DQN**: Using QNet for computing both max(a_t+1) and Q(s_t+1, a_t+1) can be biased, because it is choosing actions, then telling you how good they are with the same network. It believes too strongly in its own choices. Use two separate networks for these two computations. See https://arxiv.org/abs/1509.06461\n",
        "  * **Dueling Networks for Advantage / Value**: Use the QNet to separately predict A(s_t,a_t) and V(s_t). See: http://proceedings.mlr.press/v48/wangf16.pdf\n",
        "  * **A Distributional Perspective**: Instead of using a QNet to estimate Q(s_t,a_t), use it to paramaterize a distribution over all of the possible returns. This could be a categorical distribution. See: https://arxiv.org/pdf/1707.06887.pdf\n",
        "  \n",
        "* **Swap out CartPole-v0 for Pendulum-v0**: Since DQN is a discrete-action algorithm, you will need to use a different algorithm. Some continuous-action, off-policy algorithms you could use are:\n",
        "  * **Normalized Advantage Functions (NAF)**: https://arxiv.org/pdf/1603.00748.pdf\n",
        "  * **Deep Deterministic Policy Gradients (DDPG)**: https://arxiv.org/pdf/1509.02971.pdf"
      ]
    },
    {
      "metadata": {
        "id": "1VacJd0VNcb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title (Run This) Imports\n",
        "import os\n",
        "import gym\n",
        "import sys\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pyglet\n",
        "pyglet.options['search_local_libs']=False\n",
        "pyglet.options['shadow_window']=False\n",
        "from pyglet.window import xlib\n",
        "xlib._have_utf8 = False\n",
        "import pyglet.window\n",
        "import tensorflow as tf\n",
        "from google3.third_party.tensorflow.contrib import slim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LwPk69SQOkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title (Run This) Visualization Source Code\n",
        "# Source: https://github.com/jakevdp/JSAnimation\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "if sys.version_info < (3, 0):\n",
        "    from cStringIO import StringIO as InMemory\n",
        "else:\n",
        "    from io import BytesIO as InMemory\n",
        "from matplotlib.animation import writers, FileMovieWriter\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "JS_INCLUDE = \"\"\"\n",
        "<script language=\"javascript\">\n",
        "  /* Define the Animation class */\n",
        "  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n",
        "    this.img_id = img_id;\n",
        "    this.slider_id = slider_id;\n",
        "    this.loop_select_id = loop_select_id;\n",
        "    this.interval = interval;\n",
        "    this.current_frame = 0;\n",
        "    this.direction = 0;\n",
        "    this.timer = null;\n",
        "    this.frames = new Array(frames.length);\n",
        "\n",
        "    for (var i=0; i<frames.length; i++)\n",
        "    {\n",
        "     this.frames[i] = new Image();\n",
        "     this.frames[i].src = frames[i];\n",
        "    }\n",
        "    document.getElementById(this.slider_id).max = this.frames.length - 1;\n",
        "    this.set_frame(this.current_frame);\n",
        "  }\n",
        "\n",
        "  Animation.prototype.get_loop_state = function(){\n",
        "    var button_group = document[this.loop_select_id].state;\n",
        "    for (var i = 0; i < button_group.length; i++) {\n",
        "        var button = button_group[i];\n",
        "        if (button.checked) {\n",
        "            return button.value;\n",
        "        }\n",
        "    }\n",
        "    return undefined;\n",
        "  }\n",
        "\n",
        "  Animation.prototype.set_frame = function(frame){\n",
        "    this.current_frame = frame;\n",
        "    document.getElementById(this.img_id).src = this.frames[this.current_frame].src;\n",
        "    document.getElementById(this.slider_id).value = this.current_frame;\n",
        "  }\n",
        "\n",
        "  Animation.prototype.next_frame = function()\n",
        "  {\n",
        "    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n",
        "  }\n",
        "\n",
        "  Animation.prototype.previous_frame = function()\n",
        "  {\n",
        "    this.set_frame(Math.max(0, this.current_frame - 1));\n",
        "  }\n",
        "\n",
        "  Animation.prototype.first_frame = function()\n",
        "  {\n",
        "    this.set_frame(0);\n",
        "  }\n",
        "\n",
        "  Animation.prototype.last_frame = function()\n",
        "  {\n",
        "    this.set_frame(this.frames.length - 1);\n",
        "  }\n",
        "\n",
        "  Animation.prototype.slower = function()\n",
        "  {\n",
        "    this.interval /= 0.7;\n",
        "    if(this.direction > 0){this.play_animation();}\n",
        "    else if(this.direction < 0){this.reverse_animation();}\n",
        "  }\n",
        "\n",
        "  Animation.prototype.faster = function()\n",
        "  {\n",
        "    this.interval *= 0.7;\n",
        "    if(this.direction > 0){this.play_animation();}\n",
        "    else if(this.direction < 0){this.reverse_animation();}\n",
        "  }\n",
        "\n",
        "  Animation.prototype.anim_step_forward = function()\n",
        "  {\n",
        "    this.current_frame += 1;\n",
        "    if(this.current_frame < this.frames.length){\n",
        "      this.set_frame(this.current_frame);\n",
        "    }else{\n",
        "      var loop_state = this.get_loop_state();\n",
        "      if(loop_state == \"loop\"){\n",
        "        this.first_frame();\n",
        "      }else if(loop_state == \"reflect\"){\n",
        "        this.last_frame();\n",
        "        this.reverse_animation();\n",
        "      }else{\n",
        "        this.pause_animation();\n",
        "        this.last_frame();\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  Animation.prototype.anim_step_reverse = function()\n",
        "  {\n",
        "    this.current_frame -= 1;\n",
        "    if(this.current_frame >= 0){\n",
        "      this.set_frame(this.current_frame);\n",
        "    }else{\n",
        "      var loop_state = this.get_loop_state();\n",
        "      if(loop_state == \"loop\"){\n",
        "        this.last_frame();\n",
        "      }else if(loop_state == \"reflect\"){\n",
        "        this.first_frame();\n",
        "        this.play_animation();\n",
        "      }else{\n",
        "        this.pause_animation();\n",
        "        this.first_frame();\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  Animation.prototype.pause_animation = function()\n",
        "  {\n",
        "    this.direction = 0;\n",
        "    if (this.timer){\n",
        "      clearInterval(this.timer);\n",
        "      this.timer = null;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  Animation.prototype.play_animation = function()\n",
        "  {\n",
        "    this.pause_animation();\n",
        "    this.direction = 1;\n",
        "    var t = this;\n",
        "    if (!this.timer) this.timer = setInterval(function(){t.anim_step_forward();}, this.interval);\n",
        "  }\n",
        "\n",
        "  Animation.prototype.reverse_animation = function()\n",
        "  {\n",
        "    this.pause_animation();\n",
        "    this.direction = -1;\n",
        "    var t = this;\n",
        "    if (!this.timer) this.timer = setInterval(function(){t.anim_step_reverse();}, this.interval);\n",
        "  }\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "DISPLAY_TEMPLATE = \"\"\"\n",
        "<div class=\"animation\" align=\"center\">\n",
        "    <img id=\"_anim_img{id}\">\n",
        "    <br>\n",
        "    <input id=\"_anim_slider{id}\" type=\"range\" style=\"width:350px\" name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\" onchange=\"anim{id}.set_frame(parseInt(this.value));\"></input>\n",
        "    <br>\n",
        "    <button onclick=\"anim{id}.slower()\">&#8211;</button>\n",
        "    <button onclick=\"anim{id}.first_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/first.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.previous_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/prev.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.reverse_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/reverse.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.pause_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/pause.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.play_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/play.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.next_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/next.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.last_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/last.png?raw=true\"></button>\n",
        "    <button onclick=\"anim{id}.faster()\">+</button>\n",
        "  <form action=\"#n\" name=\"_anim_loop_select{id}\" class=\"anim_control\">\n",
        "    <input type=\"radio\" name=\"state\" value=\"once\" {once_checked}> Once </input>\n",
        "    <input type=\"radio\" name=\"state\" value=\"loop\" {loop_checked}> Loop </input>\n",
        "    <input type=\"radio\" name=\"state\" value=\"reflect\" {reflect_checked}> Reflect </input>\n",
        "  </form>\n",
        "</div>\n",
        "\n",
        "\n",
        "<script language=\"javascript\">\n",
        "  /* Instantiate the Animation class. */\n",
        "  /* The IDs given should match those used in the template above. */\n",
        "  (function() {{\n",
        "    var img_id = \"_anim_img{id}\";\n",
        "    var slider_id = \"_anim_slider{id}\";\n",
        "    var loop_select_id = \"_anim_loop_select{id}\";\n",
        "    var frames = new Array({Nframes});\n",
        "    {fill_frames}\n",
        "\n",
        "    /* set a timeout to make sure all the above elements are created before\n",
        "       the object is initialized. */\n",
        "    setTimeout(function() {{\n",
        "        anim{id} = new Animation(frames, img_id, slider_id, {interval}, loop_select_id);\n",
        "    }}, 0);\n",
        "  }})()\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "INCLUDED_FRAMES = \"\"\"\n",
        "  for (var i=0; i<{Nframes}; i++){{\n",
        "    frames[i] = \"{frame_dir}/frame\" + (\"0000000\" + i).slice(-7) + \".{frame_format}\";\n",
        "  }}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _included_frames(frame_list, frame_format):\n",
        "    \"\"\"frame_list should be a list of filenames\"\"\"\n",
        "    return INCLUDED_FRAMES.format(Nframes=len(frame_list),\n",
        "                                  frame_dir=os.path.dirname(frame_list[0]),\n",
        "                                  frame_format=frame_format)\n",
        "\n",
        "\n",
        "def _embedded_frames(frame_list, frame_format):\n",
        "    \"\"\"frame_list should be a list of base64-encoded png files\"\"\"\n",
        "    template = '  frames[{0}] = \"data:image/{1};base64,{2}\"\\n'\n",
        "    embedded = \"\\n\"\n",
        "    for i, frame_data in enumerate(frame_list):\n",
        "        embedded += template.format(i, frame_format,\n",
        "                                    frame_data.replace('\\n', '\\\\\\n'))\n",
        "    return embedded\n",
        "\n",
        "\n",
        "@writers.register('html')\n",
        "class HTMLWriter(FileMovieWriter):\n",
        "    # we start the animation id count at a random number: this way, if two\n",
        "    # animations are meant to be included on one HTML page, there is a\n",
        "    # very small chance of conflict.\n",
        "    rng = random.Random()\n",
        "    exec_key = 'animation.ffmpeg_path'\n",
        "    args_key = 'animation.ffmpeg_args'\n",
        "    supported_formats = ['png', 'jpeg', 'tiff', 'svg']\n",
        "\n",
        "    @classmethod\n",
        "    def new_id(cls):\n",
        "        #return '%16x' % cls.rng.getrandbits(64)\n",
        "        return ''.join(cls.rng.choice(string.ascii_uppercase)\n",
        "                       for x in range(16))\n",
        "\n",
        "    def __init__(self, fps=30, codec=None, bitrate=None, extra_args=None,\n",
        "                 metadata=None, embed_frames=False, default_mode='loop'):\n",
        "        self.embed_frames = embed_frames\n",
        "        self.default_mode = default_mode.lower()\n",
        "\n",
        "        if self.default_mode not in ['loop', 'once', 'reflect']:\n",
        "            self.default_mode = 'loop'\n",
        "            warnings.warn(\"unrecognized default_mode: using 'loop'\")\n",
        "\n",
        "        self._saved_frames = list()\n",
        "        super(HTMLWriter, self).__init__(fps, codec, bitrate,\n",
        "                                         extra_args, metadata)\n",
        "\n",
        "    def setup(self, fig, outfile, dpi, frame_dir=None):\n",
        "        if os.path.splitext(outfile)[-1] not in ['.html', '.htm']:\n",
        "            raise ValueError(\"outfile must be *.htm or *.html\")\n",
        "\n",
        "        if not self.embed_frames:\n",
        "            if frame_dir is None:\n",
        "                frame_dir = outfile.rstrip('.html') + '_frames'\n",
        "            if not os.path.exists(frame_dir):\n",
        "                os.makedirs(frame_dir)\n",
        "            frame_prefix = os.path.join(frame_dir, 'frame')\n",
        "        else:\n",
        "            frame_prefix = None\n",
        "\n",
        "        super(HTMLWriter, self).setup(fig, outfile, dpi,\n",
        "                                      frame_prefix, clear_temp=False)\n",
        "\n",
        "    def grab_frame(self, **savefig_kwargs):\n",
        "        if self.embed_frames:\n",
        "            suffix = '.' + self.frame_format\n",
        "            f = InMemory()\n",
        "            self.fig.savefig(f, format=self.frame_format,\n",
        "                             dpi=self.dpi, **savefig_kwargs)\n",
        "            f.seek(0)\n",
        "            self._saved_frames.append(b64encode(f.read()).decode('ascii'))\n",
        "        else:\n",
        "            return super(HTMLWriter, self).grab_frame(**savefig_kwargs)\n",
        "\n",
        "    def _run(self):\n",
        "        # make a ducktyped subprocess standin\n",
        "        # this is called by the MovieWriter base class, but not used here.\n",
        "        class ProcessStandin(object):\n",
        "            returncode = 0\n",
        "            def communicate(self):\n",
        "                return ('', '')\n",
        "        self._proc = ProcessStandin()\n",
        "\n",
        "        # save the frames to an html file\n",
        "        if self.embed_frames:\n",
        "            fill_frames = _embedded_frames(self._saved_frames,\n",
        "                                           self.frame_format)\n",
        "        else:\n",
        "            # temp names is filled by FileMovieWriter\n",
        "            fill_frames = _included_frames(self._temp_names,\n",
        "                                           self.frame_format)\n",
        "\n",
        "        mode_dict = dict(once_checked='',\n",
        "                         loop_checked='',\n",
        "                         reflect_checked='')\n",
        "        mode_dict[self.default_mode + '_checked'] = 'checked'\n",
        "\n",
        "        interval = int(1000. / self.fps)\n",
        "\n",
        "        with open(self.outfile, 'w') as of:\n",
        "            of.write(JS_INCLUDE)\n",
        "            of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n",
        "                                             Nframes=len(self._temp_names),\n",
        "                                             fill_frames=fill_frames,\n",
        "                                             interval=interval,\n",
        "                                             **mode_dict))\n",
        "            \n",
        "  # from .html_writer import HTMLWriter\n",
        "from matplotlib.animation import Animation\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "__all__ = ['anim_to_html', 'display_animation']\n",
        "\n",
        "\n",
        "class _NameOnlyTemporaryFile(object):\n",
        "    \"\"\"A context-managed temporary file which is not opened.\n",
        "\n",
        "    The file should be accessible by name on any system.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    suffix : string\n",
        "        The suffix of the temporary file (default = '')\n",
        "    prefix : string\n",
        "        The prefix of the temporary file (default = '_tmp_')\n",
        "    hash_length : string\n",
        "        The length of the random hash.  The size of the hash space will\n",
        "        be 16 ** hash_length (default=8)\n",
        "    seed : integer\n",
        "        the seed for the random number generator.  If not specified, the\n",
        "        system time will be used as a seed.\n",
        "    absolute : boolean\n",
        "        If true, return an absolute path to a temporary file in the current\n",
        "        working directory.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "\n",
        "    >>> with _NameOnlyTemporaryFile(seed=0, absolute=False) as f:\n",
        "    ...     print(f)\n",
        "    ...\n",
        "    _tmp_d82c07cd\n",
        "    >>> os.path.exists('_tmp_d82c07cd')  # file removed after context\n",
        "    False\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, prefix='_tmp_', suffix='', hash_length=8,\n",
        "                 seed=None, absolute=True):\n",
        "        rng = random.Random(seed)\n",
        "        self.name = '%s%0*x%s' % (prefix, hash_length,\n",
        "                                  rng.getrandbits(4 * hash_length), suffix)\n",
        "        if absolute:\n",
        "            self.name = os.path.abspath(self.name)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *exc_info):\n",
        "        if os.path.exists(self.name):\n",
        "            os.remove(self.name)\n",
        "\n",
        "\n",
        "def anim_to_html(anim, fps=None, embed_frames=True, default_mode='loop'):\n",
        "    \"\"\"Generate HTML representation of the animation\"\"\"\n",
        "    if fps is None and hasattr(anim, '_interval'):\n",
        "        # Convert interval in ms to frames per second\n",
        "        fps = 1000. / anim._interval\n",
        "\n",
        "    plt.close(anim._fig)\n",
        "    if hasattr(anim, \"_html_representation\"):\n",
        "        return anim._html_representation\n",
        "    else:\n",
        "        # tempfile can't be used here: we need a filename, and this\n",
        "        # fails on windows.  Instead, we use a custom filename generator\n",
        "        #with tempfile.NamedTemporaryFile(suffix='.html') as f:\n",
        "        with _NameOnlyTemporaryFile(suffix='.html') as f:\n",
        "            anim.save(f.name,  writer=HTMLWriter(fps=fps,\n",
        "                                                 embed_frames=embed_frames,\n",
        "                                                 default_mode=default_mode))\n",
        "            html = open(f.name).read()\n",
        "\n",
        "        anim._html_representation = html\n",
        "        return html\n",
        "\n",
        "\n",
        "def display_animation(anim, **kwargs):\n",
        "    \"\"\"Display the animation with an IPython HTML object\"\"\"\n",
        "    from IPython.display import HTML\n",
        "    return HTML(anim_to_html(anim, **kwargs))\n",
        "\n",
        "\n",
        "# This is the magic that makes animations display automatically in the\n",
        "# IPython notebook.  The _repr_html_ method is a special method recognized\n",
        "# by IPython.\n",
        "Animation._repr_html_ = anim_to_html\n",
        "\n",
        "%matplotlib inline\n",
        "# from JSAnimation.IPython_display import display_animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from matplotlib import animation\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy4pCYqztJV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## QNet\n",
        "\n",
        "This class implements a neural network that predicts Q-values for each action.\n",
        "\n",
        "It takes observations as input and outputs a vector of Q-values, each index corresponding to one action.\n",
        "\n",
        "The parameters of this neural net will be trained below using the DQN learning algorithm."
      ]
    },
    {
      "metadata": {
        "id": "5M5dlNDotHcA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QNet:\n",
        "  \"\"\"Simple discrete-action policy with feed-forward net.\"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def make_target_net(qnet):\n",
        "    \"\"\"Makes a clone of the given qnet, but uses the scope 'target_net'.\"\"\"\n",
        "    return QNet(qnet._obs_dim,\n",
        "                qnet._action_dim,\n",
        "                qnet._epsilon,\n",
        "                'target_' + qnet._scope)\n",
        "  \n",
        "  def __init__(self,\n",
        "               obs_dim,\n",
        "               action_dim,\n",
        "               epsilon=0.1,\n",
        "               scope='qnet'):\n",
        "    \"\"\"Create a QNet.\n",
        "    \n",
        "    Args:\n",
        "      obs_dim: The dimension of the observation vector.\n",
        "      action_dim: The number of discrete actions possible.\n",
        "      epsilon: The default probability of taking a random action, instead of the\n",
        "        action predicted to have the highest value.\n",
        "      scope: Name for tf.variable_scope for params.\n",
        "    \"\"\"\n",
        "    self._obs_dim = obs_dim\n",
        "    self._action_dim = action_dim\n",
        "    self._epsilon = epsilon\n",
        "    self._scope = scope\n",
        "    self._make_net()\n",
        "  \n",
        "  def _make_net(self):\n",
        "    # Make observation placeholder with batch dimension.\n",
        "    with tf.variable_scope(self._scope):\n",
        "      self._obs_placeholder = tf.placeholder(dtype=tf.float32, \n",
        "                                             shape=[None, self._obs_dim])\n",
        "      \n",
        "      # TODO: Create neural network layers from obs_placeholder that produce\n",
        "      #         values for action_values.\n",
        "      # Hint: Use the tf.slim library to create basic neural network layers.\n",
        "      # Hint: Pay attention to the activation_fn argument.\n",
        "      self._action_values = ???\n",
        "      \n",
        "    # Set up references to variables for getting & setting params.\n",
        "    self._variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n",
        "    self._var_placeholders = []\n",
        "    self._var_assigns = []\n",
        "    for var in self._variables:\n",
        "      var_ph = tf.placeholder(dtype=tf.float32, shape=var.shape)\n",
        "      self._var_placeholders.append(var_ph)\n",
        "      self._var_assigns.append(var.assign(var_ph))\n",
        "    \n",
        "  @property\n",
        "  def variables(self):\n",
        "    \"\"\"Property returns a list of tf.Variable that parameterize the QNet.\"\"\"\n",
        "    return self._variables\n",
        "    \n",
        "  @property\n",
        "  def action_dim(self):\n",
        "    \"\"\"The action dim of this QNet.\"\"\"\n",
        "    return self._action_dim\n",
        "  \n",
        "  @property\n",
        "  def obs_placeholder(self):\n",
        "    \"\"\"Property returns observation placeholder. shape=[batch_dim, obs_dim]\"\"\"\n",
        "    return self._obs_placeholder\n",
        "  \n",
        "  @property\n",
        "  def action_values(self):\n",
        "    \"\"\"Returns tensor of action values. shape=[batch_dim, action_dim]\"\"\"\n",
        "    return self._action_values\n",
        "  \n",
        "  def get_params(self, sess=None):\n",
        "    \"\"\"Return the parameter vals as an array of tensors, one for each param.\"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    return sess.run(self._variables)\n",
        "  \n",
        "  def set_params(self, param_vals, sess=None):\n",
        "    \"\"\"Sets policy variables equal to param_vals\"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    return sess.run(self._var_assigns,\n",
        "                    feed_dict={var_ph: param_val\n",
        "                               for (var_ph, param_val)\n",
        "                               in zip(self._var_placeholders, param_vals)})\n",
        "  \n",
        "  def sample_action(self, obs, epsilon=None, sess=None):\n",
        "    \"\"\"Samples an action from the QNet given observation.\n",
        "    \n",
        "    Args:\n",
        "      obs: Vector of observation values.\n",
        "      epsilon: Random action probability. If not specified, uses self._epsilon,\n",
        "        which defaults to 0.1.\n",
        "      sess: The session to use for execution. If not specified, uses\n",
        "        tf.get_default_session().\n",
        "    Returns:\n",
        "      An integer a: 0 <= a < action_dim.\n",
        "    \"\"\"\n",
        "    epsilon = epsilon or self._epsilon\n",
        "    \n",
        "    # TODO: Sample a float. If it is less than epsilon, return a random action, 0 <= a < action_dim.\n",
        "    # Hint: You can use numpy for this, it's a simple operation. No need for TensorFlow.\n",
        "    if ???:\n",
        "      return ???\n",
        "    \n",
        "    sess = sess or tf.get_default_session()\n",
        "    \n",
        "    # Execute the action_values op to get predictions of Q(s_t, a_t) for all possible actions.\n",
        "    action_values = sess.run(self._action_values,\n",
        "                             feed_dict={self._obs_placeholder: [obs]})\n",
        "    action_values = action_values[0] # remove the batch index\n",
        "    \n",
        "    # TODO: Return the index corresponding to the action with the highest value prediction.\n",
        "    return ???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oVRWZERfxl2B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ReplayBuffer\n",
        "\n",
        "The replay buffer is the dynamic dataset for qlearning.\n",
        "\n",
        "It gets transitions (observation_t, action_t, reward_t, observation_tp1, pcontinue) and stores them for sampling minibatches.\n",
        "* observation_t: The observation seen at time t.\n",
        "* action_t: The action taken at time t.\n",
        "* reward_t: The reward recieved after taking action_t.\n",
        "* observation_tp1: The observation at time t+1.\n",
        "* pcontinue: Float value 1.0 if episode continues after action_t, 0.0 if episode ends after action_t.\n",
        "\n",
        "Can take a minibatch of transitions from randomly shuffled history.\n"
      ]
    },
    {
      "metadata": {
        "id": "ssdiBPgmfy3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  \n",
        "  def __init__(self,\n",
        "               obs_dim,\n",
        "               action_dim,\n",
        "               size=1e5,):\n",
        "    self._obs_dim = obs_dim\n",
        "    self._action_dim = action_dim\n",
        "    self._size = size\n",
        "    \n",
        "    observations = np.zeros(shape=[size, obs_dim])\n",
        "    actions = np.zeros(shape=[size, action_dim])\n",
        "    rewards = np.zeros(shape=[size])\n",
        "    observations_tp1 = np.zeros_like(observations)\n",
        "    pcontinues = np.zeros_like(rewards)\n",
        "    self._data = (observations, actions, rewards, observations_tp1, pcontinues)\n",
        "    \n",
        "    self._write_index = -1\n",
        "    self._n = 0\n",
        "    \n",
        "  def sample_minibatch(self, batch_size=32):\n",
        "    \"\"\"Samples a minibatch of transitions from the replay buffer.\n",
        "    \n",
        "    Args:\n",
        "      batch_size: The number of transitions to pull in the batch.\n",
        "    \n",
        "    Returns:\n",
        "      A tuple of (observations, actions, rewards, observations_tp1, pcontinues).\n",
        "    \"\"\"\n",
        "    if self._n < batch_size:\n",
        "      raise IndexError('Buffer does not have batch_size=%d transitions yet.' % batch_size)\n",
        "      \n",
        "    # TODO: Sample batch_size random indeces: 0 <= i < self._n\n",
        "    # Hint: You can use numpy for this. Check out np.random.choice\n",
        "    indeces = ???\n",
        "    \n",
        "    return [array[indeces] for array in self._data]\n",
        "  \n",
        "  def add_transition(self, transition):\n",
        "    \"\"\"Adds a transition to the replay buffer.\n",
        "    \n",
        "    Args:\n",
        "      transition: A tuple of (observation, action, reward, observation_tp1, pcontinue)\n",
        "    \"\"\"\n",
        "    self._write_index = (self._write_index + 1) % self._size\n",
        "    self._n = int(min(self._size, self._n + 1))\n",
        "    for array, item in zip(self._data, transition):\n",
        "      array[self._write_index] = item\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KhfirhqAzQSp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## QLearning\n",
        "\n",
        "The QLearning object trains a QNet based on minibatches of transitions sampled from a ReplayBuffer.\n",
        "\n",
        "It uses a loss function based on TD-Error, which is the difference: Q(s_t,a_t) - [r_t + discount_factor * max_a(Q(s_t+1, a))]\n",
        "\n",
        "where the q-values are predicted by the QNet. In other words the q-value at time t should accurately predict (r_t + the value of the best action at time t+1).\n",
        "\n",
        "When we take samples from the replay buffer, this difference should be close to 0. If it's not, that is considered an error, and QLearning optimizes the parameters of QNet by minimizing that error. \n",
        "\n",
        "**Note:** QLearning also uses a copy of the QNet for computing the second Q-value. This target_net is updated much more slowly than qnet (Polyak average). This creates a more stable optimization process by changing the regression target more slowly."
      ]
    },
    {
      "metadata": {
        "id": "2LQMPAN5kdSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "  \n",
        "  def __init__(self,\n",
        "               qnet,\n",
        "               learning_rate=1e-3,\n",
        "               discount_rate=0.99,\n",
        "               target_net_update_fraction=0.05):\n",
        "    \"\"\"Create the QLearning object that trains a QNet.\n",
        "    \n",
        "    Args:\n",
        "      qnet: The QNet instance to train.\n",
        "      learning_rate: The learning rate for the optimizer.\n",
        "      discount_rate: Time discounting of rewards.\n",
        "      target_net_update_fraction: The amount of QNet updates to use for target_net updates.\n",
        "    \"\"\"\n",
        "    \n",
        "    self._qnet = qnet\n",
        "    self._learning_rate = learning_rate\n",
        "    self._discount_rate = discount_rate\n",
        "    self._target_net_update_fraction = target_net_update_fraction\n",
        "    self._target_net = QNet.make_target_net(qnet)\n",
        "    \n",
        "    \n",
        "    with tf.variable_scope('QLearning'):\n",
        "      # A one-hot mask, where the index of 1 corresponds to the action taken.\n",
        "      self._actions_taken_placeholder = tf.placeholder(dtype=tf.float32,\n",
        "                                                       shape=[None, self._qnet.action_dim],\n",
        "                                                       name='actions_taken_placeholder')\n",
        "      self._rewards_placeholder = tf.placeholder(dtype=tf.float32,\n",
        "                                                 shape=[None],\n",
        "                                                 name='rewards_placeholder')\n",
        "      self._pcontinues_placeholder = tf.placeholder(dtype=tf.float32,\n",
        "                                                 shape=[None],\n",
        "                                                 name='pcontinues_placeholder')\n",
        "      \n",
        "      # Only the action-value predictions corresponding to the action taken.\n",
        "      # This uses qnet to evaluate observation at time t.\n",
        "      # TODO: Compute the predicted action values for the actions taken, which are both shape [None, action_dim].\n",
        "      # Hint: Use self._qnet.action_values, as well as self._action_taken_placeholder.\n",
        "      # Hint: This tensor should end up being shape [None, 1].\n",
        "      self._predicted_action_values = ???\n",
        "      \n",
        "      # Max action-value predictions at timestep t+1.\n",
        "      # Uses target_net to evaluate observation_tp1.\n",
        "      # TODO: Compute the max-value of the target net output, because the target net is evaluated for observations_tp1.\n",
        "      # Hint: This tensor should end up being shape [None, 1]\n",
        "      self._next_step_best_action_values = ???\n",
        "      \n",
        "      # Regression target is r for terminal transitions. (pcontinue == 0)\n",
        "      # Regression target is r + discount_rate * max_a(Q_t+1) otherwise. (pcontinue == 1)\n",
        "      # TODO: Compute the action_value_targets.\n",
        "      # Hint: Use next_stop_best_action_values, rewards_placeholder, and pcontinues_placeholder.\n",
        "      self._action_value_targets = ???\n",
        "      \n",
        "      # TODO: The loss function should be the squared difference between predicted action values and action value targets.\n",
        "      # Hint: This should be just a scalar, shape [].\n",
        "      self._loss = ???\n",
        "      \n",
        "      self._optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
        "      \n",
        "      # Only optimize qnet's variables. Not target_net's variables.\n",
        "      self._train_op = self._optimizer.minimize(self._loss,\n",
        "                                                var_list=self._qnet.variables)\n",
        "      \n",
        "  def train(self, minibatch, sess=None):\n",
        "    \"\"\"Trains the QNet with transitions sampled from the ReplayBuffer.\n",
        "    \n",
        "    Args:\n",
        "      minibatch: A tuple of 5 numpy arrays. Each array contains the data for one of\n",
        "       obs_t, action_t, reward_t, obs_tp1, pcontinue. Transitions are aligned on the\n",
        "       first index of these arrays.\n",
        "    \"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    observations, actions, rewards, observations_tp1, pcontinues = minibatch\n",
        "\n",
        "    # Run training.\n",
        "    # TODO: Fill in the arrays with the appropriate placeholders.\n",
        "    loss, _ = sess.run([self._loss, self._train_op],\n",
        "                       feed_dict = {\n",
        "                           self._qnet.obs_placeholder: ????,\n",
        "                           self._actions_taken_placeholder: ???,\n",
        "                           self._rewards_placeholder: ???,\n",
        "                           self._target_net.obs_placeholder: ???,\n",
        "                           self._pcontinues_placeholder: ???,\n",
        "                       })\n",
        "    # Update the target net.\n",
        "    qnet_params = self._qnet.get_params(sess=sess)\n",
        "    target_net_params = self._target_net.get_params(sess=sess)\n",
        "    # TODO: Use self._target_net_update_fraction to make target_net_params an Polyak average of qnet_params.\n",
        "    for i in xrange(len(qnet_params)):\n",
        "      target_net_params[i] = (??? * qnet_params[i] +\n",
        "                              ??? * target_net_params[i])\n",
        "    self._target_net.set_params(target_net_params, sess=sess)\n",
        "    \n",
        "    return loss\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWYC5tBJyD-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Collect Episode\n",
        "\n",
        "This function collects a single episode of data. It alternates between querying environment for observations and rewards, then policy for actions given observations."
      ]
    },
    {
      "metadata": {
        "id": "qngwK9MkyB75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def collect_episode(env, policy, frames=None, replay_buffer=None, epsilon=None):\n",
        "  \"\"\"Collect an episode of environment interaction.\n",
        "  \n",
        "  Args:\n",
        "    env: The environment to interact with.\n",
        "    policy: The policy to use for action selection.\n",
        "    frames: Optional array to store the frames. If not provided, env does not\n",
        "      bother rendering.\n",
        "    replay_buffer: Optional ReplayBuffer to store environment transitions.\n",
        "  Returns:\n",
        "    Tuple of arrays of observations, actions, and rewards.\n",
        "  \"\"\"\n",
        "  obs = env.reset()\n",
        "  if frames is not None:\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "  done = False\n",
        "  observations, actions, rewards = ([], [], [])\n",
        "  while not done:\n",
        "    observations.append(obs)\n",
        "    \n",
        "    if epsilon is None:\n",
        "      action = policy.sample_action(obs)\n",
        "    else:\n",
        "      action = policy.sample_action(obs, epsilon=epsilon)\n",
        "    \n",
        "    one_hot_action = np.zeros([policy._action_dim])\n",
        "    one_hot_action[action] = 1.0\n",
        "    actions.append(one_hot_action)\n",
        "    \n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    if frames is not None:\n",
        "      frames.append(env.render(mode='rgb_array'))\n",
        "  \n",
        "  if replay_buffer is not None:\n",
        "    for i in xrange(len(observations) - 1):\n",
        "      replay_buffer.add_transition([observations[i],\n",
        "                                    actions[i],\n",
        "                                    rewards[i],\n",
        "                                    observations[i+1],\n",
        "                                    1.0])\n",
        "    replay_buffer.add_transition([observations[-1],\n",
        "                                  actions[-1],\n",
        "                                  rewards[-1],\n",
        "                                  obs,\n",
        "                                  0.0])\n",
        "  \n",
        "  return observations, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTVsIx_3xRNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Running Training\n",
        "\n",
        "This code block implements the training loop:\n",
        "* Collect EPISODES_PER_ITERATION episodes of data.\n",
        "* Use that data to train the policy.\n",
        "\n",
        "In the next code block, you can load policies from different points in training and view the episodes.\n",
        "\n",
        "### Questions\n",
        "* What happens when you vary the learning rate parameter? Does the policy learn faster? Does it change stability?\n",
        "* If you train for longer, does the policy get more stable? Does it reach a point where it never falls over? (score=200 is a perfect episode)\n",
        "* What happens when you restart training and run again? How much variance is there across runs? (may need to reduce SLIDING_WINDOW_SIZE to see a difference)"
      ]
    },
    {
      "metadata": {
        "id": "sF-ifxDyVPBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EXPERIMENT_NAME = 'qlearning' # @param\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "EPSILON = 0.1 # @param\n",
        "qnet = QNet(obs_dim=4, action_dim=2, epsilon=EPSILON)\n",
        "replay_buffer = ReplayBuffer(obs_dim=4, action_dim=2)\n",
        "\n",
        "LEARNING_RATE = 1e-3 # @param\n",
        "qlearning = QLearning(qnet,\n",
        "                      learning_rate=LEARNING_RATE,)\n",
        "\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "NUM_ITERATIONS = 1000 # @param\n",
        "EPISODES_PER_ITERATION = 1 # @param\n",
        "TEST_EPISODES_PER_ITERATION = 1 # @param\n",
        "UPDATES_PER_ITERATION = 5 # @param\n",
        "BATCH_SIZE = 64 #@param\n",
        "all_actions = []\n",
        "\n",
        "POLICY_SAVE_FREQUENCY = 20\n",
        "\n",
        "# Make dictionary for saving policy parameters.\n",
        "try:\n",
        "  policy_params[EXPERIMENT_NAME] = {}\n",
        "except NameError:\n",
        "  policy_params = {EXPERIMENT_NAME: {}}\n",
        "\n",
        "training_scores = []\n",
        "with sess.as_default():\n",
        "  for i in xrange(NUM_ITERATIONS):\n",
        "    # Collect episodes.\n",
        "    for _ in xrange(EPISODES_PER_ITERATION):\n",
        "      collect_episode(env, \n",
        "                      qnet, \n",
        "                      replay_buffer=replay_buffer)\n",
        "    # Train\n",
        "    avg_loss = 0\n",
        "    try:\n",
        "      total_loss = 0\n",
        "      for _ in xrange(UPDATES_PER_ITERATION):\n",
        "        minibatch = replay_buffer.sample_minibatch(batch_size=BATCH_SIZE)\n",
        "        total_loss += qlearning.train(minibatch)\n",
        "      avg_loss = total_loss / UPDATES_PER_ITERATION\n",
        "    except IndexError:\n",
        "      # Replay Buffer does not have enough transitions yet.\n",
        "      pass\n",
        "    \n",
        "    # Collect test episodes.\n",
        "    test_episodes = []\n",
        "    for _ in xrange(TEST_EPISODES_PER_ITERATION):\n",
        "      test_episodes.append(collect_episode(env,\n",
        "                                           qnet,\n",
        "                                           epsilon=0.0))\n",
        "    \n",
        "    # Sum rewards across episodes.\n",
        "    avg_reward = sum(map(lambda episode: sum(episode[2]),\n",
        "                           test_episodes)) / TEST_EPISODES_PER_ITERATION\n",
        "    training_scores.append(avg_reward)\n",
        "    sys.stdout.write('\\r{}/{} Iterations, {} Avg reward this iteration. {} Avg loss this iteration.'.format(i+1, NUM_ITERATIONS, avg_reward, avg_loss))\n",
        "    \n",
        "    # Save intermittent policy parameters for running episodes later.\n",
        "    if i % POLICY_SAVE_FREQUENCY == 0:\n",
        "      policy_params[EXPERIMENT_NAME][i] = qnet.get_params()        \n",
        "  \n",
        "# Avg training_scores by sliding window.\n",
        "SLIDING_WINDOW_SIZE = 30 # @param\n",
        "training_scores = [np.mean(training_scores[max(i-SLIDING_WINDOW_SIZE, 0):i+1])\n",
        "                   for i in xrange(len(training_scores))]\n",
        "\n",
        "# Save training_scores list by EXPERIMENT_NAME.\n",
        "try:\n",
        "  all_scores[EXPERIMENT_NAME] = training_scores\n",
        "except NameError:\n",
        "  all_scores = {EXPERIMENT_NAME: training_scores}\n",
        "\n",
        "# Plot all experiments so far.\n",
        "experiment_names = all_scores.keys()\n",
        "for name in experiment_names:\n",
        "  plt.plot(all_scores[name])\n",
        "plt.legend(experiment_names, loc='upper left')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSfei5kXeWmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Clear Scores\n",
        "try:\n",
        "  del all_scores\n",
        "  del policy_params\n",
        "except NameError:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6I_Mmp53scJK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Observing Episodes\n",
        "\n",
        "Load the policy parameters from different points in training, and rerun this code block to generate & visualize a new episode.\n",
        "\n",
        "NOTE: This will only work if you have started the colab kernel manually on your desktop machine. It will not work if you started it through an ssh connection."
      ]
    },
    {
      "metadata": {
        "id": "8-9F1LbcwUq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment_name = 'reinforce' # @param\n",
        "policy_version = 0 # @param\n",
        "\n",
        "# Round down to the last saved policy version.\n",
        "policy_version = int(policy_version/POLICY_SAVE_FREQUENCY) * POLICY_SAVE_FREQUENCY\n",
        "\n",
        "render_env = gym.make('CartPole-v0')\n",
        "\n",
        "try:\n",
        "  with sess.as_default():\n",
        "    qnet.set_params(policy_params[experiment_name][policy_version])\n",
        "    frames = []\n",
        "    collect_episode(render_env, policy, frames=frames)\n",
        "    display_frames_as_gif(frames)\n",
        "except e:\n",
        "  print ('DISPLAY ERROR: Cannot render episodes unless colab kernel was '\n",
        "         'manually run from corp machine.')\n",
        "  raise e\n",
        "  \n",
        "del render_env"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}